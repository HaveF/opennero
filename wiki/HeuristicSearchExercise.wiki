#summary Heuristic search exercise on IDA*
#sidebar TableOfContents

=Iterative Deepening `A*` Search=
 
In this exercise, you will be implementing the Iterative Deepening
`A*` (`IDA*`) search algorithm in the OpenNERO platform. You'll use
the existing implementations of Depth First Search (DFS) and `A*`
search as starting points, and modify and extend them to implement
`IDA*`. `IDA*` search is an informed search algorithm that uses the
same heuristic-function strategy as `A*`, but follows a DFS approach
and only keeps track of the most recent path, thus reducing the memory
complexity of the algorithm.


==The OpenNERO Platform==

The following steps should help get you up and running in OpenNERO:

  * Either:
    * Install it on your own machine by downloading one of the [http://code.google.com/p/opennero/downloads/list prebuilt binaries]
    * [BuildingOpenNero Build OpenNERO] on your own machine using the source code.
  * Read the [SystemOverview system overview] to understand the interaction between the platform and the AI agents.
  * Look at the demos for [BruteForceSearch Brute Force] (uninformed) and [HeuristicSearch Heuristic] (informed) search.


==Creating your agent==

To create your `IDA*` agent, open the [http://code.google.com/p/opennero/source/browse/trunk/mods/Maze/agent.py agent.py] file (located in <tt>trunk/mods/Maze</tt>). Notice that there are
several agents implemented here, including a DFS agent
(<tt>DFSSearchAgent</tt>) and an `A*` agent
(<tt>AStarSearchAgent</tt>).

===Agent class===
Create a new class called `IdaStarAgent` with the same code as below:

{{{
class IdaStarSearchAgent(SearchAgent):
    """
    IDA* algorithm
    """
    def __init__(self):
        # this line is crucial, otherwise the class is not recognized as an AgentBrainPtr by C++
        SearchAgent.__init__(self)

    def initialize(self, init_info):
        """
        Initializes the agent upon reset
        """

    def start(self, time, observations):
        """
        Called on the first move
        """
    
    def act(self, time, observations, reward):
        """
        Called every time the agent needs to take an action
        """

    def end(self, time, reward):
        """
        at the end of an episode, the environment tells us the final reward
        """
        print  "Final reward: %f, cumulative: %f" % (reward[0], self.fitness[0])
        self.reset()
        return True

    def destroy(self):
        """
        After one or more episodes, this agent can be disposed of
        """
        return True
}}}

Your agent is operating in a 2-D maze and is capable of moving from cell to cell. In the <tt>start</tt> and <tt>act</tt> methods, your agent receives a collection of <tt>observations</tt> about the world:

  * <tt>`observations[0]`</tt> -- The current row position of your agent.
  * <tt>`observations[1]`</tt> -- The current column position of your agent.
  * <tt>`observations[2]`</tt> -- 1 if there is an obstacle in the row above your position, otherwise 0.
  * <tt>`observations[3]`</tt> -- 1 if there is an obstacle in the row below your position, otherwise 0.
  * <tt>`observations[4]`</tt> -- 1 if there is an obstacle in the column to the right of your position, otherwise 0.
  * <tt>`observations[5]`</tt> -- 1 if there is an obstacle in the column to the left of your position, otherwise 0.

In these methods, you are required to return an <tt>action</tt> -- an integer corresponding to the desired move:

  * 0 -- Move to the row above. <tt>(row, column) &rarr; (row + 1, column)</tt>
  * 1 -- Move to the row below. <tt>(row, column) &rarr; (row - 1, column)</tt>
  * 2 -- Move to the column to the right. <tt>(row, column) &rarr; (row, column + 1)</tt>
  * 3 -- Move to the column to the left. <tt>(row, column) &rarr; (row, column - 1)</tt>
  * 4 -- Do nothing.

The class above is an empty skeleton. Refer to the other agent implementations in <tt>agent.py</tt> for guidance on how to implement in your agent. 

===Configuration file===

Once you've created your agent, you'll need to add it to the configuration file for the OpenNERO maze mod. To do this, locate the SydneyAStar.xml file (<tt>Maze/data/shapes/character/SydneyAStar.xml</tt>) and change the following line:

{{{
<Python agent="Maze.agent.AStarSearchAgent()">
}}}

to:

{{{
<Python agent="Maze.agent.IdaStarSearchAgent()">
}}}

Once you've configured this line, pressing the `Single Agent A*` button in the Maze mod will run your agent.

===Debugging===

If you run into any bugs in your program, you can find the error log file for OpenNERO at one of the following locations:

  * *Linux or Mac:* `~/.opennero/nero_log.txt`
  * *Windows:* `"AppData\Local\OpenNERO\nero_log.txt"` or `"Local Settings\Application Data\OpenNERO\nero_log.txt"` depending on the version of Windows you have.